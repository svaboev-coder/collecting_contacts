import json
import logging
from typing import List, Dict, Any, Tuple

try:
    # –ó–∞–ø—É—Å–∫ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ backend (python backend/main.py)
    from utils import contact_extractor, scraper
except ImportError:
    # –ó–∞–ø—É—Å–∫ –∫–∞–∫ –ø–∞–∫–µ—Ç (uvicorn backend.main:app)
    from backend.utils import contact_extractor, scraper

logger = logging.getLogger(__name__)


class ContactAgent:
    """–ü—Ä–æ—Å—Ç–æ–π –ø—Å–µ–≤–¥–æ–∞–≥–µ–Ω—Ç (ReAct-—Å—Ç–∏–ª—å): –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí –¥–µ–π—Å—Ç–≤–∏–µ (–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç) ‚Üí –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Üí —Ä–µ—Ñ–ª–µ–∫—Å–∏—è.

    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
    - fetch_url: —Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ URL –∏ –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã
    - extract_from_text: –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    - finalize: –≤–µ—Ä–Ω—É—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π JSON —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏
    """

    def __init__(self, proxy_client, model: str = "claude-3-5-sonnet-20240620", max_steps: int = 6):
        self.proxy_client = proxy_client
        self.model = model
        self.max_steps = max_steps

    async def run(self, location: str) -> Tuple[List[Dict[str, str]], List[str]]:
        logs: List[str] = []
        scratchpad: List[Dict[str, Any]] = []
        collected_contacts: List[Dict[str, str]] = []

        system_spec = (
            "–¢—ã ‚Äî –∞–≥–µ–Ω—Ç –ø–æ —Å–±–æ—Ä—É –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –æ—Ç–µ–ª–µ–π/–±–∞–∑ –æ—Ç–¥—ã—Ö–∞/—Å–∞–Ω–∞—Ç–æ—Ä–∏–µ–≤."
            " –£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: fetch_url, extract_from_text, finalize."
            " –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ: {\n"
            "  \"action\": \"fetch_url|extract_from_text|finalize\",\n"
            "  \"input\": string –∏–ª–∏ –æ–±—ä–µ–∫—Ç,\n"
            "  \"reason\": string\n"
            "}. –ë–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞."
        )

        def build_messages() -> List[Dict[str, str]]:
            context = {
                "goal": f"–°–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –º–µ—Å—Ç —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤ {location}.",
                "tools": [
                    {"name": "fetch_url", "args": "{url}", "desc": "—Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã"},
                    {"name": "extract_from_text", "args": "{text}", "desc": "–∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"},
                    {"name": "finalize", "args": "‚Äî", "desc": "–≤–µ—Ä–Ω—É—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π JSON —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏"},
                ],
                "history": scratchpad,
            }
            user_prompt = (
                "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n" + json.dumps(context, ensure_ascii=False, indent=2) +
                "\n–°–¥–µ–ª–∞–π —Å–ª–µ–¥—É—é—â–∏–π –Ω–∞–∏–ª—É—á—à–∏–π —à–∞–≥."
            )
            return [
                {"role": "user", "content": system_spec + "\n\n" + user_prompt},
            ]

        for step in range(1, self.max_steps + 1):
            logs.append(f"ü§î –®–∞–≥ –∞–≥–µ–Ω—Ç–∞ {step}: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ")
            messages = build_messages()
            try:
                resp = await self.proxy_client.chat_completion(
                    model=self.model,
                    messages=messages,
                    max_tokens=800,
                    temperature=0.2,
                )
                content = resp["choices"][0]["message"]["content"]
                logs.append(f"üß† –ü–ª–∞–Ω: {content[:200]}...")
                try:
                    decision = json.loads(content)
                except Exception:
                    logs.append("‚ö†Ô∏è –ê–≥–µ–Ω—Ç –≤–µ—Ä–Ω—É–ª –Ω–µ-JSON. –ü–æ–≤—Ç–æ—Ä —à–∞–≥–∞.")
                    scratchpad.append({"thought": content})
                    continue

                action = (decision.get("action") or "").strip()
                action_input = decision.get("input")
                reason = decision.get("reason") or ""
                scratchpad.append({"action": action, "input": action_input, "reason": reason})

                if action == "fetch_url":
                    url = (action_input or "").strip()
                    if not url:
                        logs.append("‚ö†Ô∏è –ü—É—Å—Ç–æ–π URL —É –¥–µ–π—Å—Ç–≤–∏—è fetch_url")
                        continue
                    logs.append(f"üåê fetch_url: {url}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ContactExtractor –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    try:
                        page_data = contact_extractor.extract_contacts_from_url(url)
                        logs.append(f"üîé –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {str(page_data)[:200]}...")
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ (–ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ email/–∞–¥—Ä–µ—Å–∞)
                        new_candidates = self._contacts_from_extraction(page_data)
                        collected_contacts.extend(new_candidates)
                        scratchpad.append({"observation": {"extracted": page_data, "new_contacts": new_candidates}})
                    except Exception as e:
                        logs.append(f"‚ùå –û—à–∏–±–∫–∞ fetch_url: {e}")
                        scratchpad.append({"observation": {"error": str(e)}})

                elif action == "extract_from_text":
                    text = action_input if isinstance(action_input, str) else json.dumps(action_input, ensure_ascii=False)
                    logs.append(f"üìù extract_from_text: {text[:200]}...")
                    extracted = contact_extractor.extract_contacts_from_text(text)
                    new_candidates = self._contacts_from_extraction(extracted)
                    collected_contacts.extend(new_candidates)
                    scratchpad.append({"observation": {"extracted": extracted, "new_contacts": new_candidates}})

                elif action == "finalize":
                    logs.append("‚úÖ –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–º")
                    # –û—Ç–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã
                    final_contacts = self._dedupe_contacts(collected_contacts)
                    return final_contacts, logs

                else:
                    logs.append(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞: {action}")
                    continue

                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –æ–±—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–µ—Å–ª–∏ —ç—Ç–æ URL)
                if action == "fetch_url" and isinstance(action_input, str):
                    base_url = action_input
                    links = scraper.get_links(base_url, max_links=10)
                    if links:
                        logs.append(f"üîó –ù–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ ({len(links)}): {links[:5]}...")
                        for link in links[:5]:
                            page_data = contact_extractor.extract_contacts_from_url(link)
                            new_candidates = self._contacts_from_extraction(page_data)
                            if new_candidates:
                                # –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –∏–º—è
                                name = scraper.get_title(link)
                                for nc in new_candidates:
                                    if not nc.get('name'):
                                        nc['name'] = name
                                collected_contacts.extend(new_candidates)
                        scratchpad.append({"observation": {"crawled_links": len(links), "collected": len(collected_contacts)}})

            except Exception as e:
                logs.append(f"‚ùå –û—à–∏–±–∫–∞ —à–∞–≥–∞ –∞–≥–µ–Ω—Ç–∞: {e}")
                continue

        # –•–∞—Ä–¥-—Å—Ç–æ–ø: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º, —á—Ç–æ –Ω–∞—à–ª–∏
        logs.append("‚èπ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —à–∞–≥–æ–≤. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ.")
        return self._dedupe_contacts(collected_contacts), logs

    def _contacts_from_extraction(self, data: Dict[str, Any]) -> List[Dict[str, str]]:
        candidates: List[Dict[str, str]] = []
        emails = data.get("emails") or []
        addresses = data.get("addresses") or []
        websites = data.get("websites") or []
        # –°–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤; –∏–º—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏–º, –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ LLM'–æ–º –Ω–∞ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        for email in emails:
            candidates.append({
                "name": "",
                "address": "",
                "coordinates": "",
                "email": email,
                "website": "",
            })
        for addr in addresses:
            candidates.append({
                "name": "",
                "address": addr,
                "coordinates": "",
                "email": "",
                "website": "",
            })
        for site in websites:
            candidates.append({
                "name": "",
                "address": "",
                "coordinates": "",
                "email": "",
                "website": site,
            })
        return candidates

    def _dedupe_contacts(self, contacts: List[Dict[str, str]]) -> List[Dict[str, str]]:
        seen = set()
        unique: List[Dict[str, str]] = []
        for c in contacts:
            key = (c.get("name") or "", c.get("address") or "", c.get("email") or "", c.get("website") or "")
            if key in seen:
                continue
            seen.add(key)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ–ª—è
            normalized = {
                "name": (c.get("name") or "").strip(),
                "address": (c.get("address") or "").strip(),
                "coordinates": (c.get("coordinates") or "").strip(),
                "email": (c.get("email") or "").strip(),
                "website": (c.get("website") or "").strip(),
            }
            unique.append(normalized)
        return unique


