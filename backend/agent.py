import json
import logging
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urlparse, urljoin

try:
    # –ó–∞–ø—É—Å–∫ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ backend (python backend/main.py)
    from utils import contact_extractor, scraper, web_search, yandex_search
except ImportError:
    # –ó–∞–ø—É—Å–∫ –∫–∞–∫ –ø–∞–∫–µ—Ç (uvicorn backend.main:app)
    from backend.utils import contact_extractor, scraper, web_search, yandex_search

logger = logging.getLogger(__name__)


class ContactAgent:
    """–ü—Ä–æ—Å—Ç–æ–π –ø—Å–µ–≤–¥–æ–∞–≥–µ–Ω—Ç (ReAct-—Å—Ç–∏–ª—å): –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí –¥–µ–π—Å—Ç–≤–∏–µ (–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç) ‚Üí –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Üí —Ä–µ—Ñ–ª–µ–∫—Å–∏—è.

    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
    - fetch_url: —Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ URL –∏ –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã
    - extract_from_text: –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    - finalize: –≤–µ—Ä–Ω—É—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π JSON —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏
    """

    def __init__(self, proxy_client, model: str = "claude-3-5-sonnet-20240620", max_steps: int = 6):
        self.proxy_client = proxy_client
        self.model = model
        self.max_steps = max_steps

    async def run(self, location: str) -> Tuple[List[Dict[str, str]], List[str]]:
        logs: List[str] = []
        scratchpad: List[Dict[str, Any]] = []
        collected_contacts: List[Dict[str, str]] = []

        system_spec = (
            "–¢—ã ‚Äî –∞–≥–µ–Ω—Ç –ø–æ —Å–±–æ—Ä—É –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –æ—Ç–µ–ª–µ–π/–±–∞–∑ –æ—Ç–¥—ã—Ö–∞/—Å–∞–Ω–∞—Ç–æ—Ä–∏–µ–≤."
            " –£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: fetch_url, extract_from_text, finalize."
            " –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ: {\n"
            "  \"action\": \"fetch_url|extract_from_text|finalize\",\n"
            "  \"input\": string –∏–ª–∏ –æ–±—ä–µ–∫—Ç,\n"
            "  \"reason\": string\n"
            "}. –ë–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞."
        )

        def build_messages() -> List[Dict[str, str]]:
            context = {
                "goal": f"–°–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –º–µ—Å—Ç —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤ {location}.",
                "tools": [
                    {"name": "fetch_url", "args": "{url}", "desc": "—Å–∫–∞—á–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã"},
                    {"name": "extract_from_text", "args": "{text}", "desc": "–∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"},
                    {"name": "finalize", "args": "‚Äî", "desc": "–≤–µ—Ä–Ω—É—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π JSON —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏"},
                ],
                "history": scratchpad,
            }
            user_prompt = (
                "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n" + json.dumps(context, ensure_ascii=False, indent=2) +
                "\n–°–¥–µ–ª–∞–π —Å–ª–µ–¥—É—é—â–∏–π –Ω–∞–∏–ª—É—á—à–∏–π —à–∞–≥."
            )
            return [
                {"role": "user", "content": system_spec + "\n\n" + user_prompt},
            ]

        for step in range(1, self.max_steps + 1):
            logs.append(f"ü§î –®–∞–≥ –∞–≥–µ–Ω—Ç–∞ {step}: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ")
            messages = build_messages()
            try:
                resp = await self.proxy_client.chat_completion(
                    model=self.model,
                    messages=messages,
                    max_tokens=800,
                    temperature=0.2,
                )
                content = resp["choices"][0]["message"]["content"]
                logs.append(f"üß† –ü–ª–∞–Ω: {content[:200]}...")
                try:
                    decision = json.loads(content)
                except Exception:
                    logs.append("‚ö†Ô∏è –ê–≥–µ–Ω—Ç –≤–µ—Ä–Ω—É–ª –Ω–µ-JSON. –ü–æ–≤—Ç–æ—Ä —à–∞–≥–∞.")
                    scratchpad.append({"thought": content})
                    continue

                action = (decision.get("action") or "").strip()
                action_input = decision.get("input")
                reason = decision.get("reason") or ""
                scratchpad.append({"action": action, "input": action_input, "reason": reason})

                if action == "fetch_url":
                    url = (action_input or "").strip()
                    if not url:
                        logs.append("‚ö†Ô∏è –ü—É—Å—Ç–æ–π URL —É –¥–µ–π—Å—Ç–≤–∏—è fetch_url")
                        continue
                    logs.append(f"üåê fetch_url: {url}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ContactExtractor –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    try:
                        page_data = contact_extractor.extract_contacts_from_url(url)
                        logs.append(f"üîé –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {str(page_data)[:200]}...")
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ (–ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ email/–∞–¥—Ä–µ—Å–∞)
                        new_candidates = self._contacts_from_extraction(page_data)
                        collected_contacts.extend(new_candidates)
                        scratchpad.append({"observation": {"extracted": page_data, "new_contacts": new_candidates}})
                    except Exception as e:
                        logs.append(f"‚ùå –û—à–∏–±–∫–∞ fetch_url: {e}")
                        scratchpad.append({"observation": {"error": str(e)}})

                elif action == "extract_from_text":
                    text = action_input if isinstance(action_input, str) else json.dumps(action_input, ensure_ascii=False)
                    logs.append(f"üìù extract_from_text: {text[:200]}...")
                    extracted = contact_extractor.extract_contacts_from_text(text)
                    new_candidates = self._contacts_from_extraction(extracted)
                    collected_contacts.extend(new_candidates)
                    scratchpad.append({"observation": {"extracted": extracted, "new_contacts": new_candidates}})

                elif action == "finalize":
                    logs.append("‚úÖ –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–º")
                    # –û—Ç–¥–∞–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã
                    final_contacts = self._dedupe_contacts(collected_contacts)
                    return final_contacts, logs

                else:
                    logs.append(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞: {action}")
                    continue

                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –æ–±—Ö–æ–¥–∏–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–µ—Å–ª–∏ —ç—Ç–æ URL)
                if action == "fetch_url" and isinstance(action_input, str):
                    base_url = action_input
                    links = scraper.get_links(base_url, max_links=10)
                    if links:
                        logs.append(f"üîó –ù–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ ({len(links)}): {links[:5]}...")
                        for link in links[:5]:
                            page_data = contact_extractor.extract_contacts_from_url(link)
                            new_candidates = self._contacts_from_extraction(page_data)
                            if new_candidates:
                                # –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –∏–º—è
                                name = scraper.get_title(link)
                                for nc in new_candidates:
                                    if not nc.get('name'):
                                        nc['name'] = name
                                collected_contacts.extend(new_candidates)
                        scratchpad.append({"observation": {"crawled_links": len(links), "collected": len(collected_contacts)}})

            except Exception as e:
                logs.append(f"‚ùå –û—à–∏–±–∫–∞ —à–∞–≥–∞ –∞–≥–µ–Ω—Ç–∞: {e}")
                continue

        # –•–∞—Ä–¥-—Å—Ç–æ–ø: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º, —á—Ç–æ –Ω–∞—à–ª–∏
        logs.append("‚èπ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —à–∞–≥–æ–≤. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ.")
        return self._dedupe_contacts(collected_contacts), logs

    def _contacts_from_extraction(self, data: Dict[str, Any]) -> List[Dict[str, str]]:
        candidates: List[Dict[str, str]] = []
        emails = data.get("emails") or []
        addresses = data.get("addresses") or []
        websites = data.get("websites") or []
        # –°–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤; –∏–º—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏–º, –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ LLM'–æ–º –Ω–∞ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        for email in emails:
            candidates.append({
                "name": "",
                "address": "",
                "coordinates": "",
                "email": email,
                "website": "",
            })
        for addr in addresses:
            candidates.append({
                "name": "",
                "address": addr,
                "coordinates": "",
                "email": "",
                "website": "",
            })
        for site in websites:
            candidates.append({
                "name": "",
                "address": "",
                "coordinates": "",
                "email": "",
                "website": site,
            })
        return candidates

    def _dedupe_contacts(self, contacts: List[Dict[str, str]]) -> List[Dict[str, str]]:
        seen = set()
        unique: List[Dict[str, str]] = []
        for c in contacts:
            key = (c.get("name") or "", c.get("address") or "", c.get("email") or "", c.get("website") or "")
            if key in seen:
                continue
            seen.add(key)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ–ª—è
            normalized = {
                "name": (c.get("name") or "").strip(),
                "address": (c.get("address") or "").strip(),
                "coordinates": (c.get("coordinates") or "").strip(),
                "email": (c.get("email") or "").strip(),
                "website": (c.get("website") or "").strip(),
            }
            unique.append(normalized)
        return unique



class WebsiteFinderAgent:
    """–ê–≥–µ–Ω—Ç –ø–æ–¥–±–æ—Ä–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏ –≥–æ—Ä–æ–¥—É.

    –ê–ª–≥–æ—Ä–∏—Ç–º (–±–µ–∑ LLM):
      1) –ó–∞–ø—Ä–æ—Å—ã –≤ –≤–µ–±‚Äë–ø–æ–∏—Å–∫ (DuckDuckGo/Bing) –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º
      2) –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
      3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: –∑–∞–≥–æ–ª–æ–≤–æ–∫/title, –Ω–∞–ª–∏—á–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤, —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –Ω–∞–∑–≤–∞–Ω–∏—è, —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
      4) –°–∫–æ–∏–Ω–≥ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    """

    def __init__(self, max_candidates_per_query: int = 12, max_checked_pages: int = 10, proxy_client: Optional[object] = None):
        self.max_candidates_per_query = max_candidates_per_query
        self.max_checked_pages = max_checked_pages
        self.proxy_client = proxy_client

    def _normalize_text(self, text: str) -> str:
        return ''.join(ch.lower() for ch in (text or '') if ch.isalnum() or ch.isspace()).strip()

    def _important_tokens(self, name: str) -> List[str]:
        stop_words = {
            '–≥–æ—Å—Ç–µ–≤–æ–π', '–¥–æ–º', '–≥–æ—Å—Ç–∏–Ω–∏—Ü–∞', 'hotel', 'hostel', '–≥–æ—Å—Ç–µ–≤–æ–π–¥–æ–º', '–æ—Ç–µ–ª—å', '—Å–∞–Ω–∞—Ç–æ—Ä–∏–π',
            '–ø–∞–Ω—Å–∏–æ–Ω–∞—Ç', 'resort', '–±–∞–∑–∞', '–æ—Ç–¥—ã—Ö–∞', '–∞–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã', 'apartments', '–º–∏–Ω–∏', '–º–∏–Ω–∏–æ—Ç–µ–ª—å'
        }
        tokens = [t for t in self._normalize_text(name).split() if t]
        return [t for t in tokens if t not in stop_words and len(t) > 2]

    def _transliterate_ru_to_lat(self, text: str) -> str:
        mapping = {
            '–∞':'a','–±':'b','–≤':'v','–≥':'g','–¥':'d','–µ':'e','—ë':'yo','–∂':'zh','–∑':'z','–∏':'i','–π':'y',
            '–∫':'k','–ª':'l','–º':'m','–Ω':'n','–æ':'o','–ø':'p','—Ä':'r','—Å':'s','—Ç':'t','—É':'u','—Ñ':'f',
            '—Ö':'kh','—Ü':'ts','—á':'ch','—à':'sh','—â':'shch','—ä':'','—ã':'y','—å':'','—ç':'e','—é':'yu','—è':'ya'
        }
        res = []
        for ch in (text or '').lower():
            res.append(mapping.get(ch, ch))
        return ''.join(res)

    def _score_candidate(self, *,
                         url: str,
                         title: str,
                         page_text: str,
                         location: str,
                         name_tokens: List[str]) -> int:
        score = 0
        low_title = (title or '').lower()
        low_text = (page_text or '').lower()
        low_url = (url or '').lower()

        # 1) –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ –¥–æ–º–µ–Ω–µ –∏–ª–∏ —Ç–∞–π—Ç–ª–µ
        for t in name_tokens:
            if t and t in low_title:
                score += 2
            # –≥—Ä—É–±–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ URL
            if f"/{t}" in low_url or t in low_url.split('/')[-1]:
                score += 1

        # 2) –ì–æ—Ä–æ–¥ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        if self._normalize_text(location) and self._normalize_text(location) in self._normalize_text(low_text):
            score += 1

        # 2.1) –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –≥–æ—Ä–æ–¥–∞/–Ω–∞–∑–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥–æ–º–µ–Ω–µ/URL
        loc_lat = self._transliterate_ru_to_lat(location)
        if loc_lat and loc_lat in low_url:
            score += 3

        # 2.2) "–æ—Ñ–∏—Ü–∏–∞–ª—å" –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —É—Å–∏–ª–∏–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if '–æ—Ñ–∏—Ü–∏–∞–ª' in low_title or 'official' in low_title:
            score += 2

        # 3) –ù–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–ª–∏ —Ä–µ–∫–≤–∏–∑–∏—Ç–æ–≤
        if any(k in low_text for k in ['–∫–æ–Ω—Ç–∞–∫—Ç', '—Ç–µ–ª.', '—Ç–µ–ª–µ—Ñ–æ–Ω', 'email', '–ø–æ—á—Ç–∞', '–∏–Ω–Ω', '–æ–≥—Ä–Ω', '¬©']):
            score += 2

        # 4) –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        contact_links = scraper.get_links(url, max_links=10, keywords=['contact', 'contacts', '–∫–æ–Ω—Ç–∞–∫—Ç', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', 'about', '–æ-–∫–æ–º–ø–∞–Ω–∏–∏'])
        if contact_links:
            score += 2

        return score

    def _root_domain(self, url: str) -> str:
        try:
            host = urlparse(url).netloc.lower()
            if host.startswith('www.'):
                host = host[4:]
            parts = host.split('.')
            if len(parts) >= 2:
                return '.'.join(parts[-2:])  # –≤—Ç–æ—Ä–æ–π —É—Ä–æ–≤–µ–Ω—å
            return host
        except Exception:
            return ''

    def _probe_contact_page(self, base_url: str, location: str) -> Tuple[bool, str]:
        """–ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –¥–æ–º–µ–Ω–µ –∏ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ç–∞–º –µ—Å—Ç—å –≥–æ—Ä–æ–¥/–∫–æ–Ω—Ç–∞–∫—Ç—ã."""
        try:
            candidates = [
                '/contacts', '/contact', '/kontakty', '/kontact', '/kontaktyi', '/kontaktyi/',
                '/–∫–æ–Ω—Ç–∞–∫—Ç—ã', '/–∫–æ–Ω—Ç–∞–∫—Ç', '/–æ-–∫–æ–º–ø–∞–Ω–∏–∏', '/–æ_–∫–æ–º–ø–∞–Ω–∏–∏', '/about', '/about-us', '/o-kompanii', '/o_kompanii', '/o-nas', '/o_nas'
            ]
            base = f"{urlparse(base_url).scheme}://{urlparse(base_url).netloc}"
            for path in candidates:
                url = urljoin(base, path)
                text = scraper.get_page_content(url) or ''
                if not text:
                    continue
                low = text.lower()
                if self._normalize_text(location) and self._normalize_text(location) not in self._normalize_text(low):
                    # –µ—Å–ª–∏ –Ω–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≥–æ—Ä–æ–¥–∞ ‚Äî —Å–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª, –ø–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    continue
                # –µ—Å—Ç—å –≥–æ—Ä–æ–¥ ‚Äî —É—Å–∏–ª–∏–º –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
                extracted = contact_extractor.extract_contacts_from_text(text)
                has_any = bool((extracted.get('emails') or []) or (extracted.get('phones') or []) or (extracted.get('addresses') or []))
                if has_any:
                    return True, url
            return False, ''
        except Exception:
            return False, ''

    def _llm_pick_best(self, location: str, name: str, details: List[Dict[str, str]], logs: List[str]) -> Optional[str]:
        try:
            if not self.proxy_client:
                return None
            # –°–æ—Å—Ç–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫–∏–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è LLM
            items = []
            for d in details[:10]:
                items.append({
                    "url": d.get("url", ""),
                    "title": (d.get("title", "") or "")[:120],
                    "score": d.get("score", 0),
                    "contact_found": bool(d.get("contact_found")),
                })
            prompt = (
                "–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.\n"
                "–î–∞–Ω–æ: –≥–æ—Ä–æ–¥: '" + location + "', –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: '" + name + "'.\n"
                "–°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (url/title/score/contact_found).\n"
                "–í—ã–±–µ—Ä–∏ –æ–¥–∏–Ω –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –∏ –æ—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ JSON –≤–∏–¥–∞ {\"url\": string, \"reason\": string}.\n"
                "–ö—Ä–∏—Ç–µ—Ä–∏–∏: –¥–æ–º–µ–Ω 2-–≥–æ —É—Ä–æ–≤–Ω—è, –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∞–¥—Ä–µ—Å–æ–º –≤ —ç—Ç–æ–º –≥–æ—Ä–æ–¥–µ, –±—Ä–µ–Ω–¥/–Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ, –∏—Å–∫–ª—é—á–∏—Ç—å –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã."
            )
            messages = [{"role": "user", "content": prompt + "\n\n–ö–∞–Ω–¥–∏–¥–∞—Ç—ã:\n" + json.dumps(items, ensure_ascii=False)}]
            resp = self.proxy_client.chat_completion(
                model="claude-3-5-sonnet-20240620",
                messages=messages,
                max_tokens=250,
                temperature=0
            )
            # –ú–æ–∂–µ—Ç –±—ã—Ç—å sync/await; –Ω–∞—à ProxyAPIClient async. –ü–æ–ø—Ä–æ–±—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä —á–µ—Ä–µ–∑ getattr
            if hasattr(self.proxy_client, "chat_completion") and callable(getattr(self.proxy_client, "chat_completion")):
                # –í–æ–∑–º–æ–∂–Ω–æ –≤—ã—à–µ –≤—ã–∑–æ–≤ –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—É—Ç–∏–Ω—É
                if hasattr(resp, "__await__"):
                    import asyncio
                    resp = asyncio.get_event_loop().run_until_complete(resp)
            content = resp["choices"][0]["message"]["content"] if isinstance(resp, dict) else ""
            try:
                data = json.loads(content)
                url = (data.get("url") or "").strip()
                if url:
                    logs.append(f"ü§ñ LLM –≤—ã–±—Ä–∞–ª: {url}")
                    return url
            except Exception:
                logs.append("‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ-JSON")
                return None
            return None
        except Exception as e:
            logs.append(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM-–æ—Ü–µ–Ω–∫–∏: {e}")
            return None

    def _pick_best(self, location: str, name: str, candidates: List[str], logs: List[str]) -> Optional[str]:
        name_tokens = self._important_tokens(name)
        best_url: Optional[str] = None
        best_score = -1
        checked = 0
        checked_domains = set()
        details: List[Dict[str, object]] = []
        for url in candidates:
            if checked >= self.max_checked_pages:
                break
            try:
                # –î–µ–¥—É–ø –ø–æ –∫–æ—Ä–Ω–µ–≤–æ–º—É –¥–æ–º–µ–Ω—É
                rd = self._root_domain(url)
                if rd and rd in checked_domains:
                    continue
                # –û—Ç—Å–µ—á—å —Ç—Ä—ë—Ö- –∏ –±–æ–ª–µ–µ —É—Ä–æ–≤–Ω–µ–≤—ã–µ –¥–æ–º–µ–Ω—ã —É –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–æ–≤
                host = urlparse(url).netloc.lower()
                if host.count('.') >= 2:
                    # –µ—Å–ª–∏ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–æ–º–µ–Ω –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    if any(rd.endswith(agg) for agg in ['broniryem.ru', 'booking.com', 'ostrovok.ru', '101hotels.ru', '101hotels.com']):
                        continue
                checked_domains.add(rd)
                title = scraper.get_title(url)
                text = scraper.get_page_content(url) or ''
                score = self._score_candidate(url=url, title=title, page_text=text, location=location, name_tokens=name_tokens)
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                is_official, contact_url = self._probe_contact_page(url, location)
                if is_official:
                    score += 6
                    logs.append(f"‚úÖ –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞–π–¥–µ–Ω–∞: {contact_url}")
                logs.append(f"üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ {url} | title='{title[:80] if title else ''}' | score={score}")
                details.append({"url": url, "title": title or "", "score": score, "contact_found": is_official})
                if score > best_score:
                    best_score = score
                    best_url = url
            except Exception as e:
                logs.append(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
            finally:
                checked += 1
        # –ü–æ—Ä–æ–≥, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        if best_url and best_score >= 3:
            return best_url
        # –ü–æ–ø—Ä–æ–±—É–µ–º LLM-–¥–æ–æ—Ü–µ–Ω–∫—É
        llm_choice = self._llm_pick_best(location, ' '.join(name_tokens) or name, details, logs)
        return llm_choice

    def find_official_website(self, location: str, name: str) -> Tuple[Optional[str], List[str]]:
        logs: List[str] = []
        logs.append(f"üîç –ü–æ–∏—Å–∫ —Å–∞–π—Ç–∞ –¥–ª—è '{name}' –≤ '{location}' (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –Ø–Ω–¥–µ–∫—Å)")
        # 1) –°–Ω–∞—á–∞–ª–∞ –Ø–Ω–¥–µ–∫—Å Search API
        try:
            agg = list(getattr(web_search, 'aggregator_domains', getattr(scraper, 'aggregator_domains', [])) or [])
        except Exception:
            agg = []
        ys = yandex_search.find_website(name, location, aggregator_domains=agg)
        if ys:
            logs.append(f"üü¢ –Ø–Ω–¥–µ–∫—Å: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–∞–π–¥–µ–Ω: {ys}")
            logs.append(f"‚úÖ –í—ã–±—Ä–∞–Ω —Å–∞–π—Ç: {ys}")
            return ys, logs
        # 2) –ï—Å–ª–∏ –Ø–Ω–¥–µ–∫—Å –≤–µ—Ä–Ω—É–ª 401/403 ‚Äî —Ñ–æ–ª–±—ç–∫ –Ω–∞ –≤–µ–±‚Äë–ø–æ–∏—Å–∫, —á—Ç–æ–±—ã –Ω–µ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try:
            code = getattr(yandex_search, 'last_status_code', None)
            dbg = getattr(yandex_search, 'last_debug', None)
            if dbg:
                for line in dbg[:20]:
                    logs.append(f"YANDEX DBG: {line}")
            if code in (401, 403):
                logs.append(f"üü† –Ø–Ω–¥–µ–∫—Å –æ—Ç–∫–∞–∑–∞–ª (HTTP {code}). –ü–µ—Ä–µ—Ö–æ–∂—É –∫ –≤–µ–±‚Äë–ø–æ–∏—Å–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.")
                # –ì–æ—Ç–æ–≤–∏–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–≤–∫–ª—é—á–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—é)
                translit_name = self._transliterate_ru_to_lat(name)
                translit_loc = self._transliterate_ru_to_lat(location)
                q1 = f"{name} {location} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
                q2 = f"{name} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç {location}"
                q3 = f"{name} {location} —Å–∞–π—Ç"
                q4 = f"{translit_name} {translit_loc} official site"
                q5 = f"{translit_name} {translit_loc} site"
                q6 = f"{translit_name} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
                q7 = f"{location} {name} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π"
                results: List[str] = []
                seen = set()
                for q in (q1, q2, q3, q4, q5, q6, q7):
                    urls = web_search.search(q, max_results=self.max_candidates_per_query)
                    logs.append(f"üß≠ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã '{q}': {len(urls)}")
                    for u in urls:
                        key = (u.split('#')[0])
                        if key in seen:
                            continue
                        seen.add(key)
                        results.append(u)
                if results:
                    best = self._pick_best(location, name, results, logs)
                    if best:
                        logs.append(f"‚úÖ –í—ã–±—Ä–∞–Ω —Å–∞–π—Ç (—Ñ–æ–ª–±—ç–∫): {best}")
                        return best, logs
                logs.append("‚ùå –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω (–ø–æ—Å–ª–µ —Ñ–æ–ª–±—ç–∫–∞)")
                return None, logs
        except Exception:
            pass
        logs.append("üü° –Ø–Ω–¥–µ–∫—Å –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç. –ü–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é ‚Äî –ø—Ä–µ–∫—Ä–∞—â–∞—é –ø–æ–∏—Å–∫.")
        logs.append("‚ùå –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None, logs

    def find_for_names(self, location: str, names: List[str]) -> Tuple[List[Dict[str, str]], List[str]]:
        all_logs: List[str] = []
        results: List[Dict[str, str]] = []
        for name in names:
            site, logs = self.find_official_website(location, name)
            all_logs.extend(logs)
            results.append({"name": name, "website": site or ""})
        return results, all_logs

