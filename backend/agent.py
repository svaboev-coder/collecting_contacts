import json
import logging
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urlparse, urljoin

try:
    # Ð—Ð°Ð¿ÑƒÑÐº Ð¸Ð· Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ backend (python backend/main.py)
    from utils import contact_extractor, scraper, web_search, yandex_search
except ImportError:
    # Ð—Ð°Ð¿ÑƒÑÐº ÐºÐ°Ðº Ð¿Ð°ÐºÐµÑ‚ (uvicorn backend.main:app)
    from backend.utils import contact_extractor, scraper, web_search, yandex_search

logger = logging.getLogger(__name__)


class ContactAgent:
    """ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿ÑÐµÐ²Ð´Ð¾Ð°Ð³ÐµÐ½Ñ‚ (ReAct-ÑÑ‚Ð¸Ð»ÑŒ): Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ â†’ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ (Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚) â†’ Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ðµ â†’ Ñ€ÐµÑ„Ð»ÐµÐºÑÐ¸Ñ.

    Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹:
    - fetch_url: ÑÐºÐ°Ñ‡Ð°Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð¿Ð¾ URL Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹
    - extract_from_text: Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°
    - finalize: Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ JSON Ñ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð°Ð¼Ð¸
    """

    def __init__(self, proxy_client, model: str = "claude-3-5-sonnet-20240620", max_steps: int = 6):
        self.proxy_client = proxy_client
        self.model = model
        self.max_steps = max_steps

    async def run(self, location: str) -> Tuple[List[Dict[str, str]], List[str]]:
        logs: List[str] = []
        scratchpad: List[Dict[str, Any]] = []
        collected_contacts: List[Dict[str, str]] = []

        system_spec = (
            "Ð¢Ñ‹ â€” Ð°Ð³ÐµÐ½Ñ‚ Ð¿Ð¾ ÑÐ±Ð¾Ñ€Ñƒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð² Ð¾Ñ‚ÐµÐ»ÐµÐ¹/Ð±Ð°Ð· Ð¾Ñ‚Ð´Ñ‹Ñ…Ð°/ÑÐ°Ð½Ð°Ñ‚Ð¾Ñ€Ð¸ÐµÐ²."
            " Ð£ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹: fetch_url, extract_from_text, finalize."
            " Ð’ÑÐµÐ³Ð´Ð° Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹ ÑÑ‚Ñ€Ð¾Ð³Ð¾ JSON Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ: {\n"
            "  \"action\": \"fetch_url|extract_from_text|finalize\",\n"
            "  \"input\": string Ð¸Ð»Ð¸ Ð¾Ð±ÑŠÐµÐºÑ‚,\n"
            "  \"reason\": string\n"
            "}. Ð‘ÐµÐ· Ð»Ð¸ÑˆÐ½ÐµÐ³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°."
        )

        def build_messages() -> List[Dict[str, str]]:
            context = {
                "goal": f"Ð¡Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹ Ð¼ÐµÑÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ‰ÐµÐ½Ð¸Ñ Ð² {location}.",
                "tools": [
                    {"name": "fetch_url", "args": "{url}", "desc": "ÑÐºÐ°Ñ‡Ð°Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹"},
                    {"name": "extract_from_text", "args": "{text}", "desc": "Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°"},
                    {"name": "finalize", "args": "â€”", "desc": "Ð²ÐµÑ€Ð½ÑƒÑ‚ÑŒ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ JSON Ñ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð°Ð¼Ð¸"},
                ],
                "history": scratchpad,
            }
            user_prompt = (
                "ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n" + json.dumps(context, ensure_ascii=False, indent=2) +
                "\nÐ¡Ð´ÐµÐ»Ð°Ð¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ð½Ð°Ð¸Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ ÑˆÐ°Ð³."
            )
            return [
                {"role": "user", "content": system_spec + "\n\n" + user_prompt},
            ]

        for step in range(1, self.max_steps + 1):
            logs.append(f"ðŸ¤” Ð¨Ð°Ð³ Ð°Ð³ÐµÐ½Ñ‚Ð° {step}: Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ")
            messages = build_messages()
            try:
                resp = await self.proxy_client.chat_completion(
                    model=self.model,
                    messages=messages,
                    max_tokens=800,
                    temperature=0.2,
                )
                content = resp["choices"][0]["message"]["content"]
                logs.append(f"ðŸ§  ÐŸÐ»Ð°Ð½: {content[:200]}...")
                try:
                    decision = json.loads(content)
                except Exception:
                    logs.append("âš ï¸ ÐÐ³ÐµÐ½Ñ‚ Ð²ÐµÑ€Ð½ÑƒÐ» Ð½Ðµ-JSON. ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€ ÑˆÐ°Ð³Ð°.")
                    scratchpad.append({"thought": content})
                    continue

                action = (decision.get("action") or "").strip()
                action_input = decision.get("input")
                reason = decision.get("reason") or ""
                scratchpad.append({"action": action, "input": action_input, "reason": reason})

                if action == "fetch_url":
                    url = (action_input or "").strip()
                    if not url:
                        logs.append("âš ï¸ ÐŸÑƒÑÑ‚Ð¾Ð¹ URL Ñƒ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ fetch_url")
                        continue
                    logs.append(f"ðŸŒ fetch_url: {url}")
                    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ContactExtractor Ð´Ð»Ñ Ð¿ÐµÑ€Ð²Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ
                    try:
                        page_data = contact_extractor.extract_contacts_from_url(url)
                        logs.append(f"ðŸ”Ž Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ: {str(page_data)[:200]}...")
                        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð² (Ð¿Ð¾ÐºÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ email/Ð°Ð´Ñ€ÐµÑÐ°)
                        new_candidates = self._contacts_from_extraction(page_data)
                        collected_contacts.extend(new_candidates)
                        scratchpad.append({"observation": {"extracted": page_data, "new_contacts": new_candidates}})
                    except Exception as e:
                        logs.append(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° fetch_url: {e}")
                        scratchpad.append({"observation": {"error": str(e)}})

                elif action == "extract_from_text":
                    text = action_input if isinstance(action_input, str) else json.dumps(action_input, ensure_ascii=False)
                    logs.append(f"ðŸ“ extract_from_text: {text[:200]}...")
                    extracted = contact_extractor.extract_contacts_from_text(text)
                    new_candidates = self._contacts_from_extraction(extracted)
                    collected_contacts.extend(new_candidates)
                    scratchpad.append({"observation": {"extracted": extracted, "new_contacts": new_candidates}})

                elif action == "finalize":
                    logs.append("âœ… Ð¤Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð¼")
                    # ÐžÑ‚Ð´Ð°ÐµÐ¼ Ð°Ð³Ñ€ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ, ÑƒÐ½Ð¸ÐºÐ°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹
                    final_contacts = self._dedupe_contacts(collected_contacts)
                    return final_contacts, logs

                else:
                    logs.append(f"âš ï¸ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ Ð°Ð³ÐµÐ½Ñ‚Ð°: {action}")
                    continue

                # Ð•ÑÐ»Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ â€” Ð¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ (ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ URL)
                if action == "fetch_url" and isinstance(action_input, str):
                    base_url = action_input
                    links = scraper.get_links(base_url, max_links=10)
                    if links:
                        logs.append(f"ðŸ”— ÐÐ°Ð¹Ð´ÐµÐ½Ñ‹ ÑÑÑ‹Ð»ÐºÐ¸ Ð´Ð»Ñ Ð¾Ð±Ñ…Ð¾Ð´Ð° ({len(links)}): {links[:5]}...")
                        for link in links[:5]:
                            page_data = contact_extractor.extract_contacts_from_url(link)
                            new_candidates = self._contacts_from_extraction(page_data)
                            if new_candidates:
                                # Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº ÐºÐ°Ðº Ð¸Ð¼Ñ
                                name = scraper.get_title(link)
                                for nc in new_candidates:
                                    if not nc.get('name'):
                                        nc['name'] = name
                                collected_contacts.extend(new_candidates)
                        scratchpad.append({"observation": {"crawled_links": len(links), "collected": len(collected_contacts)}})

            except Exception as e:
                logs.append(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑˆÐ°Ð³Ð° Ð°Ð³ÐµÐ½Ñ‚Ð°: {e}")
                continue

        # Ð¥Ð°Ñ€Ð´-ÑÑ‚Ð¾Ð¿: Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð½Ð°ÑˆÐ»Ð¸
        logs.append("â¹ Ð”Ð¾ÑÑ‚Ð¸Ð³Ð½ÑƒÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚ ÑˆÐ°Ð³Ð¾Ð². Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ð¾Ðµ.")
        return self._dedupe_contacts(collected_contacts), logs

    def _contacts_from_extraction(self, data: Dict[str, Any]) -> List[Dict[str, str]]:
        candidates: List[Dict[str, str]] = []
        emails = data.get("emails") or []
        addresses = data.get("addresses") or []
        websites = data.get("websites") or []
        # Ð¡ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐ¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²; Ð¸Ð¼Ñ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾ â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼, Ð±ÑƒÐ´ÐµÑ‚ Ð·Ð°Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾ Ð¿Ð¾Ð·Ð¶Ðµ LLM'Ð¾Ð¼ Ð½Ð° Ñ„Ð¸Ð½Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        for email in emails:
            candidates.append({
                "name": "",
                "address": "",
                "coordinates": "",
                "email": email,
                "website": "",
            })
        for addr in addresses:
            candidates.append({
                "name": "",
                "address": addr,
                "coordinates": "",
                "email": "",
                "website": "",
            })
        for site in websites:
            candidates.append({
                "name": "",
                "address": "",
                "coordinates": "",
                "email": "",
                "website": site,
            })
        return candidates

    def _dedupe_contacts(self, contacts: List[Dict[str, str]]) -> List[Dict[str, str]]:
        seen = set()
        unique: List[Dict[str, str]] = []
        for c in contacts:
            key = (c.get("name") or "", c.get("address") or "", c.get("email") or "", c.get("website") or "")
            if key in seen:
                continue
            seen.add(key)
            # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ Ð¿Ð¾Ð»Ñ
            normalized = {
                "name": (c.get("name") or "").strip(),
                "address": (c.get("address") or "").strip(),
                "coordinates": (c.get("coordinates") or "").strip(),
                "email": (c.get("email") or "").strip(),
                "website": (c.get("website") or "").strip(),
            }
            unique.append(normalized)
        return unique



class WebsiteFinderAgent:
    """ÐÐ³ÐµÐ½Ñ‚ Ð¿Ð¾Ð´Ð±Ð¾Ñ€Ð° Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ°Ð¹Ñ‚Ð¾Ð² Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ Ð¸ Ð³Ð¾Ñ€Ð¾Ð´Ñƒ.

    ÐÐ»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ (Ð±ÐµÐ· LLM):
      1) Ð—Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð² Ð²ÐµÐ±â€‘Ð¿Ð¾Ð¸ÑÐº (DuckDuckGo/Bing) Ð¿Ð¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ°Ð¼
      2) Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ñ€Ð¾Ð² Ð¸ Ð½ÐµÑ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð²
      3) ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²: Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº/title, Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð², ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð°Ð¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ, ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð°
      4) Ð¡ÐºÐ¾Ð¸Ð½Ð³ Ð¸ Ð²Ñ‹Ð±Ð¾Ñ€ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð°
    """

    def __init__(self, max_candidates_per_query: int = 12, max_checked_pages: int = 10, proxy_client: Optional[object] = None):
        self.max_candidates_per_query = max_candidates_per_query
        self.max_checked_pages = max_checked_pages
        self.proxy_client = proxy_client

    def _normalize_text(self, text: str) -> str:
        return ''.join(ch.lower() for ch in (text or '') if ch.isalnum() or ch.isspace()).strip()

    def _important_tokens(self, name: str) -> List[str]:
        stop_words = {
            'Ð³Ð¾ÑÑ‚ÐµÐ²Ð¾Ð¹', 'Ð´Ð¾Ð¼', 'Ð³Ð¾ÑÑ‚Ð¸Ð½Ð¸Ñ†Ð°', 'hotel', 'hostel', 'Ð³Ð¾ÑÑ‚ÐµÐ²Ð¾Ð¹Ð´Ð¾Ð¼', 'Ð¾Ñ‚ÐµÐ»ÑŒ', 'ÑÐ°Ð½Ð°Ñ‚Ð¾Ñ€Ð¸Ð¹',
            'Ð¿Ð°Ð½ÑÐ¸Ð¾Ð½Ð°Ñ‚', 'resort', 'Ð±Ð°Ð·Ð°', 'Ð¾Ñ‚Ð´Ñ‹Ñ…Ð°', 'Ð°Ð¿Ð°Ñ€Ñ‚Ð°Ð¼ÐµÐ½Ñ‚Ñ‹', 'apartments', 'Ð¼Ð¸Ð½Ð¸', 'Ð¼Ð¸Ð½Ð¸Ð¾Ñ‚ÐµÐ»ÑŒ'
        }
        tokens = [t for t in self._normalize_text(name).split() if t]
        return [t for t in tokens if t not in stop_words and len(t) > 2]

    def _transliterate_ru_to_lat(self, text: str) -> str:
        mapping = {
            'Ð°':'a','Ð±':'b','Ð²':'v','Ð³':'g','Ð´':'d','Ðµ':'e','Ñ‘':'yo','Ð¶':'zh','Ð·':'z','Ð¸':'i','Ð¹':'y',
            'Ðº':'k','Ð»':'l','Ð¼':'m','Ð½':'n','Ð¾':'o','Ð¿':'p','Ñ€':'r','Ñ':'s','Ñ‚':'t','Ñƒ':'u','Ñ„':'f',
            'Ñ…':'kh','Ñ†':'ts','Ñ‡':'ch','Ñˆ':'sh','Ñ‰':'shch','ÑŠ':'','Ñ‹':'y','ÑŒ':'','Ñ':'e','ÑŽ':'yu','Ñ':'ya'
        }
        res = []
        for ch in (text or '').lower():
            res.append(mapping.get(ch, ch))
        return ''.join(res)

    def _score_candidate(self, *,
                         url: str,
                         title: str,
                         page_text: str,
                         location: str,
                         name_tokens: List[str]) -> int:
        score = 0
        low_title = (title or '').lower()
        low_text = (page_text or '').lower()
        low_url = (url or '').lower()

        # 1) Ð¡Ð¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ð² Ð´Ð¾Ð¼ÐµÐ½Ðµ Ð¸Ð»Ð¸ Ñ‚Ð°Ð¹Ñ‚Ð»Ðµ
        for t in name_tokens:
            if t and t in low_title:
                score += 2
            # Ð³Ñ€ÑƒÐ±Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ð¾ URL
            if f"/{t}" in low_url or t in low_url.split('/')[-1]:
                score += 1

        # 2) Ð“Ð¾Ñ€Ð¾Ð´ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ
        if self._normalize_text(location) and self._normalize_text(location) in self._normalize_text(low_text):
            score += 1

        # 2.1) Ð¢Ñ€Ð°Ð½ÑÐ»Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ Ð³Ð¾Ñ€Ð¾Ð´Ð°/Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ð² Ð´Ð¾Ð¼ÐµÐ½Ðµ/URL
        loc_lat = self._transliterate_ru_to_lat(location)
        if loc_lat and loc_lat in low_url:
            score += 3

        # 2.2) "Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒ" Ð² Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐµ ÑƒÑÐ¸Ð»Ð¸Ð²Ð°ÐµÑ‚ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ
        if 'Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»' in low_title or 'official' in low_title:
            score += 2

        # 3) ÐÐ°Ð»Ð¸Ñ‡Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð² ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð² Ð¸Ð»Ð¸ Ñ€ÐµÐºÐ²Ð¸Ð·Ð¸Ñ‚Ð¾Ð²
        if any(k in low_text for k in ['ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚', 'Ñ‚ÐµÐ».', 'Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½', 'email', 'Ð¿Ð¾Ñ‡Ñ‚Ð°', 'Ð¸Ð½Ð½', 'Ð¾Ð³Ñ€Ð½', 'Â©']):
            score += 2

        # 4) ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸
        contact_links = scraper.get_links(url, max_links=10, keywords=['contact', 'contacts', 'ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚', 'ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹', 'about', 'Ð¾-ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸'])
        if contact_links:
            score += 2

        return score

    def _root_domain(self, url: str) -> str:
        try:
            host = urlparse(url).netloc.lower()
            if host.startswith('www.'):
                host = host[4:]
            parts = host.split('.')
            if len(parts) >= 2:
                return '.'.join(parts[-2:])  # Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ
            return host
        except Exception:
            return ''

    def _probe_contact_page(self, base_url: str, location: str) -> Tuple[bool, str]:
        """ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð½Ð°Ð¹Ñ‚Ð¸ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½ÑƒÑŽ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð½Ð° Ð´Ð¾Ð¼ÐµÐ½Ðµ Ð¸ ÑƒÐ±ÐµÐ´Ð¸Ñ‚ÑŒÑÑ, Ñ‡Ñ‚Ð¾ Ñ‚Ð°Ð¼ ÐµÑÑ‚ÑŒ Ð³Ð¾Ñ€Ð¾Ð´/ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹."""
        try:
            candidates = [
                '/contacts', '/contact', '/kontakty', '/kontact', '/kontaktyi', '/kontaktyi/',
                '/ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹', '/ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚', '/Ð¾-ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸', '/Ð¾_ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸', '/about', '/about-us', '/o-kompanii', '/o_kompanii', '/o-nas', '/o_nas'
            ]
            base = f"{urlparse(base_url).scheme}://{urlparse(base_url).netloc}"
            for path in candidates:
                url = urljoin(base, path)
                text = scraper.get_page_content(url) or ''
                if not text:
                    continue
                low = text.lower()
                if self._normalize_text(location) and self._normalize_text(location) not in self._normalize_text(low):
                    # ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ Ð³Ð¾Ñ€Ð¾Ð´Ð° â€” ÑÐ»Ð°Ð±Ñ‹Ð¹ ÑÐ¸Ð³Ð½Ð°Ð», Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð´Ñ€ÑƒÐ³Ð¸Ðµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
                    continue
                # ÐµÑÑ‚ÑŒ Ð³Ð¾Ñ€Ð¾Ð´ â€” ÑƒÑÐ¸Ð»Ð¸Ð¼ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¾Ð¹ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð²
                extracted = contact_extractor.extract_contacts_from_text(text)
                has_any = bool((extracted.get('emails') or []) or (extracted.get('phones') or []) or (extracted.get('addresses') or []))
                if has_any:
                    return True, url
            return False, ''
        except Exception:
            return False, ''

    def _llm_pick_best(self, location: str, name: str, details: List[Dict[str, str]], logs: List[str]) -> Optional[str]:
        try:
            if not self.proxy_client:
                return None
            # Ð¡Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð´Ð»Ñ LLM
            items = []
            for d in details[:10]:
                items.append({
                    "url": d.get("url", ""),
                    "title": (d.get("title", "") or "")[:120],
                    "score": d.get("score", 0),
                    "contact_found": bool(d.get("contact_found")),
                })
            prompt = (
                "Ð¢Ñ‹ Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑˆÑŒ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸.\n"
                "Ð”Ð°Ð½Ð¾: Ð³Ð¾Ñ€Ð¾Ð´: '" + location + "', Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ: '" + name + "'.\n"
                "Ð¡Ð¿Ð¸ÑÐ¾Ðº ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² (url/title/score/contact_found).\n"
                "Ð’Ñ‹Ð±ÐµÑ€Ð¸ Ð¾Ð´Ð¸Ð½ Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚ Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ÑŒ ÑÑ‚Ñ€Ð¾Ð³Ð¾ JSON Ð²Ð¸Ð´Ð° {\"url\": string, \"reason\": string}.\n"
                "ÐšÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸: Ð´Ð¾Ð¼ÐµÐ½ 2-Ð³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ, Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ñ Ð°Ð´Ñ€ÐµÑÐ¾Ð¼ Ð² ÑÑ‚Ð¾Ð¼ Ð³Ð¾Ñ€Ð¾Ð´Ðµ, Ð±Ñ€ÐµÐ½Ð´/Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ðµ, Ð¸ÑÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ñ€Ñ‹."
            )
            messages = [{"role": "user", "content": prompt + "\n\nÐšÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ñ‹:\n" + json.dumps(items, ensure_ascii=False)}]
            resp = self.proxy_client.chat_completion(
                model="claude-3-5-sonnet-20240620",
                messages=messages,
                max_tokens=250,
                temperature=0
            )
            # ÐœÐ¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ sync/await; Ð½Ð°Ñˆ ProxyAPIClient async. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€ Ñ‡ÐµÑ€ÐµÐ· getattr
            if hasattr(self.proxy_client, "chat_completion") and callable(getattr(self.proxy_client, "chat_completion")):
                # Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ñ‹ÑˆÐµ Ð²Ñ‹Ð·Ð¾Ð² Ð²ÐµÑ€Ð½ÑƒÐ» ÐºÐ¾Ñ€ÑƒÑ‚Ð¸Ð½Ñƒ
                if hasattr(resp, "__await__"):
                    import asyncio
                    resp = asyncio.get_event_loop().run_until_complete(resp)
            content = resp["choices"][0]["message"]["content"] if isinstance(resp, dict) else ""
            try:
                data = json.loads(content)
                url = (data.get("url") or "").strip()
                if url:
                    logs.append(f"ðŸ¤– LLM Ð²Ñ‹Ð±Ñ€Ð°Ð»: {url}")
                    return url
            except Exception:
                logs.append("âš ï¸ LLM Ð²ÐµÑ€Ð½ÑƒÐ» Ð½Ðµ-JSON")
                return None
            return None
        except Exception as e:
            logs.append(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° LLM-Ð¾Ñ†ÐµÐ½ÐºÐ¸: {e}")
            return None

    def _pick_best(self, location: str, name: str, candidates: List[str], logs: List[str]) -> Optional[str]:
        name_tokens = self._important_tokens(name)
        best_url: Optional[str] = None
        best_score = -1
        checked = 0
        checked_domains = set()
        details: List[Dict[str, object]] = []
        for url in candidates:
            if checked >= self.max_checked_pages:
                break
            try:
                # Ð”ÐµÐ´ÑƒÐ¿ Ð¿Ð¾ ÐºÐ¾Ñ€Ð½ÐµÐ²Ð¾Ð¼Ñƒ Ð´Ð¾Ð¼ÐµÐ½Ñƒ
                rd = self._root_domain(url)
                if rd and rd in checked_domains:
                    continue
                # ÐžÑ‚ÑÐµÑ‡ÑŒ Ñ‚Ñ€Ñ‘Ñ…- Ð¸ Ð±Ð¾Ð»ÐµÐµ ÑƒÑ€Ð¾Ð²Ð½ÐµÐ²Ñ‹Ðµ Ð´Ð¾Ð¼ÐµÐ½Ñ‹ Ñƒ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ñ€Ð¾Ð²
                host = urlparse(url).netloc.lower()
                if host.count('.') >= 2:
                    # ÐµÑÐ»Ð¸ ÐºÐ¾Ñ€Ð½ÐµÐ²Ð¾Ð¹ Ð´Ð¾Ð¼ÐµÐ½ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾Ð³Ð¾ Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ñ€Ð° â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼
                    if any(rd.endswith(agg) for agg in ['broniryem.ru', 'booking.com', 'ostrovok.ru', '101hotels.ru', '101hotels.com']):
                        continue
                checked_domains.add(rd)
                title = scraper.get_title(url)
                text = scraper.get_page_content(url) or ''
                score = self._score_candidate(url=url, title=title, page_text=text, location=location, name_tokens=name_tokens)
                # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
                is_official, contact_url = self._probe_contact_page(url, location)
                if is_official:
                    score += 6
                    logs.append(f"âœ… ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ð°Ñ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {contact_url}")
                logs.append(f"ðŸ”Ž ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° {url} | title='{title[:80] if title else ''}' | score={score}")
                details.append({"url": url, "title": title or "", "score": score, "contact_found": is_official})
                if score > best_score:
                    best_score = score
                    best_url = url
            except Exception as e:
                logs.append(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ {url}: {e}")
            finally:
                checked += 1
        # ÐŸÐ¾Ñ€Ð¾Ð³, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð»Ð¾Ð¶Ð½Ñ‹Ñ… ÑÑ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ð¹
        if best_url and best_score >= 3:
            return best_url
        # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ LLM-Ð´Ð¾Ð¾Ñ†ÐµÐ½ÐºÑƒ
        llm_choice = self._llm_pick_best(location, ' '.join(name_tokens) or name, details, logs)
        return llm_choice

    def find_official_website(self, location: str, name: str) -> Tuple[Optional[str], List[str]]:
        logs: List[str] = []
        logs.append(f"ðŸ” ÐŸÐ¾Ð¸ÑÐº ÑÐ°Ð¹Ñ‚Ð° Ð´Ð»Ñ '{name}' Ð² '{location}' (Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚: Ð¯Ð½Ð´ÐµÐºÑ)")
        # 1) Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¯Ð½Ð´ÐµÐºÑ Search API
        try:
            agg = list(getattr(web_search, 'aggregator_domains', getattr(scraper, 'aggregator_domains', [])) or [])
        except Exception:
            agg = []
        ys = yandex_search.find_website(name, location, aggregator_domains=agg)
        if ys:
            logs.append(f"ðŸŸ¢ Ð¯Ð½Ð´ÐµÐºÑ: Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚ Ð½Ð°Ð¹Ð´ÐµÐ½: {ys}")
            logs.append(f"âœ… Ð’Ñ‹Ð±Ñ€Ð°Ð½ ÑÐ°Ð¹Ñ‚: {ys}")
            return ys, logs
        # 2) Ð•ÑÐ»Ð¸ Ð¯Ð½Ð´ÐµÐºÑ Ð²ÐµÑ€Ð½ÑƒÐ» 401/403 â€” Ñ„Ð¾Ð»Ð±ÑÐº Ð½Ð° Ð²ÐµÐ±â€‘Ð¿Ð¾Ð¸ÑÐº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¾ÑÑ‚Ð°Ð²Ð°Ñ‚ÑŒÑÑ Ð±ÐµÐ· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°
        try:
            code = getattr(yandex_search, 'last_status_code', None)
            dbg = getattr(yandex_search, 'last_debug', None)
            if dbg:
                for line in dbg[:20]:
                    logs.append(f"YANDEX DBG: {line}")
            if code in (401, 403):
                logs.append(f"ðŸŸ  Ð¯Ð½Ð´ÐµÐºÑ Ð¾Ñ‚ÐºÐ°Ð·Ð°Ð» (HTTP {code}). ÐŸÐµÑ€ÐµÑ…Ð¾Ð¶Ñƒ Ðº Ð²ÐµÐ±â€‘Ð¿Ð¾Ð¸ÑÐºÑƒ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð².")
                # Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ (Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ñ‚Ñ€Ð°Ð½ÑÐ»Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑŽ)
                translit_name = self._transliterate_ru_to_lat(name)
                translit_loc = self._transliterate_ru_to_lat(location)
                q1 = f"{name} {location} Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚"
                q2 = f"{name} Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚ {location}"
                q3 = f"{name} {location} ÑÐ°Ð¹Ñ‚"
                q4 = f"{translit_name} {translit_loc} official site"
                q5 = f"{translit_name} {translit_loc} site"
                q6 = f"{translit_name} Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚"
                q7 = f"{location} {name} Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹"
                results: List[str] = []
                seen = set()
                for q in (q1, q2, q3, q4, q5, q6, q7):
                    urls = web_search.search(q, max_results=self.max_candidates_per_query)
                    logs.append(f"ðŸ§­ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ '{q}': {len(urls)}")
                    for u in urls:
                        key = (u.split('#')[0])
                        if key in seen:
                            continue
                        seen.add(key)
                        results.append(u)
                if results:
                    best = self._pick_best(location, name, results, logs)
                    if best:
                        logs.append(f"âœ… Ð’Ñ‹Ð±Ñ€Ð°Ð½ ÑÐ°Ð¹Ñ‚ (Ñ„Ð¾Ð»Ð±ÑÐº): {best}")
                        return best, logs
                logs.append("âŒ ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ (Ð¿Ð¾ÑÐ»Ðµ Ñ„Ð¾Ð»Ð±ÑÐºÐ°)")
                return None, logs
        except Exception:
            pass
        logs.append("ðŸŸ¡ Ð¯Ð½Ð´ÐµÐºÑ Ð½Ðµ Ð²ÐµÑ€Ð½ÑƒÐ» Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚. ÐŸÐ¾ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑŽ â€” Ð¿Ñ€ÐµÐºÑ€Ð°Ñ‰Ð°ÑŽ Ð¿Ð¾Ð¸ÑÐº.")
        logs.append("âŒ ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ°Ð¹Ñ‚ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        return None, logs

    def find_for_names(self, location: str, names: List[str]) -> Tuple[List[Dict[str, str]], List[str]]:
        all_logs: List[str] = []
        results: List[Dict[str, str]] = []
        for name in names:
            site, logs = self.find_official_website(location, name)
            all_logs.extend(logs)
            results.append({"name": name, "website": site or ""})
        return results, all_logs

