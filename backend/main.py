from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import Response
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, field_validator
import requests
from bs4 import BeautifulSoup
import json
import os
from dotenv import load_dotenv
import hashlib

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()
import asyncio
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime, timedelta
import pandas as pd
from pathlib import Path
import tempfile
try:
    # –ó–∞–ø—É—Å–∫ –∏–∑ –ø–∞–ø–∫–∏ backend (python backend/main.py)
    from utils import contact_extractor, scraper, name_finder, contacts_crawler
    from agent import ContactAgent, WebsiteFinderAgent
    from proxy_api import ProxyAPIClient
    from cache_manager import cache_manager, CacheData, Organization, ProcessStatus
except ImportError:
    # –ó–∞–ø—É—Å–∫ –∫–∞–∫ –ø–∞–∫–µ—Ç (uvicorn backend.main:app)
    from backend.utils import contact_extractor, scraper, name_finder, contacts_crawler
    from backend.agent import ContactAgent, WebsiteFinderAgent
    from backend.proxy_api import ProxyAPIClient
    from backend.cache_manager import cache_manager, CacheData, Organization, ProcessStatus

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ + –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å)
LOG_PATH = '/app/backend_debug.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(LOG_PATH, encoding='utf-8')
    ],
    force=True,
)
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[1]
BACKEND_DIR = Path(__file__).resolve().parent

app = FastAPI(title="–ö–æ–Ω—Ç–∞–∫—Ç—ã –æ—Ç–µ–ª–µ–π", version="1.0.0")
logger.info(f"üìù –õ–æ–≥ –ø–∏—à–µ—Ç—Å—è –≤ —Ñ–∞–π–ª: {LOG_PATH}")

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–∞–∑–≤–∞–Ω–∏–π (—Ñ–∞–π–ª–æ–≤–æ–µ)
NAMES_STORE_DIR = Path(__file__).resolve().parent / 'names_store'
try:
    NAMES_STORE_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π: {NAMES_STORE_DIR}")
except Exception as e:
    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π: {e}")


class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        try:
            client_host = request.client.host if request.client else "?"
            logger.info(f"‚û°Ô∏è {request.method} {request.url.path}{'?' + request.url.query if request.url.query else ''} from {client_host}")
            body_bytes = await request.body()
            if body_bytes:
                body_preview = body_bytes.decode('utf-8', errors='ignore')[:2000]
                logger.info(f"üßæ Request body: {body_preview}")

            async def receive():
                return {"type": "http.request", "body": body_bytes, "more_body": False}

            request._receive = receive

            response = await call_next(request)

            resp_body = b""
            async for chunk in response.body_iterator:
                resp_body += chunk
            resp_preview = resp_body.decode('utf-8', errors='ignore')[:2000]
            logger.info(f"‚¨ÖÔ∏è {response.status_code} {request.method} {request.url.path} body: {resp_preview}")

            return Response(content=resp_body,
                            status_code=response.status_code,
                            headers=dict(response.headers),
                            media_type=response.media_type)
        except Exception as e:
            logger.error(f"LoggingMiddleware error: {e}")
            return await call_next(request)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –í–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤/–æ—Ç–≤–µ—Ç–æ–≤
app.add_middleware(LoggingMiddleware)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ProxyAPI –∫–ª–∏–µ–Ω—Ç–∞
try:
    proxy_client = ProxyAPIClient()
    logger.info("‚úÖ ProxyAPI –∫–ª–∏–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    try:
        from proxy_api import ProxyAPIClient as _P
        # –≤—ã–≤–µ–¥–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π base_url, –µ—Å–ª–∏ –µ—Å—Ç—å
        logger.info(f"üåê ProxyAPI BASE URL: {proxy_client.base_url}")
    except Exception:
        pass
except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ProxyAPI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
    proxy_client = None

# –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
results_cache = {}
CACHE_EXPIRY_HOURS = 24

class LocationRequest(BaseModel):
    location: str
    
    @field_validator('location')
    @classmethod
    def validate_location(cls, v):
        if not v or len(v.strip()) < 2:
            raise ValueError('–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞—Å–µ–ª–µ–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞')
        if len(v.strip()) > 100:
            raise ValueError('–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞—Å–µ–ª–µ–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ')
        return v.strip()

class ContactInfo(BaseModel):
    name: str
    address: str
    coordinates: str
    email: str
    website: str

class CollectionStep(BaseModel):
    step: int
    description: str
    prompt: str

class CollectionPlan(BaseModel):
    steps: List[CollectionStep]

class CollectionResult(BaseModel):
    logs: List[str]
    contacts: List[ContactInfo]
    timestamp: str
    location: str

class NameListResult(BaseModel):
    location: str
    names: List[str]
    timestamp: str

class WebsiteFindRequest(BaseModel):
    location: str
    names: List[str]

    @field_validator('location')
    @classmethod
    def _validate_location(cls, v):
        if not v or len(v.strip()) < 2:
            raise ValueError('–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞—Å–µ–ª–µ–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 2 —Å–∏–º–≤–æ–ª–∞')
        if len(v.strip()) > 100:
            raise ValueError('–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞—Å–µ–ª–µ–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ')
        return v.strip()

class WebsiteItem(BaseModel):
    name: str
    website: str

class WebsiteFindResult(BaseModel):
    location: str
    items: List[WebsiteItem]
    logs: List[str]
    timestamp: str

class WebsiteExportItem(BaseModel):
    name: str
    website: str
    email: str = ""
    address: str = ""

class WebsiteExportRequest(BaseModel):
    location: str
    items: List[WebsiteExportItem]

# –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫—ç—à–µ–º
class CacheStatusResponse(BaseModel):
    location_match: bool
    next_stage: str
    process_status: Dict[str, Any]
    organizations_count: int
    cache_data: Optional[Dict[str, Any]] = None

class CacheUpdateRequest(BaseModel):
    location: str
    stage: str  # 'names', 'websites', 'contacts'
    status: str  # 'completed', 'interrupted', 'not_started'
    organizations: Optional[List[Dict[str, str]]] = None


class ContactExtractItem(BaseModel):
    name: str
    website: str

class ContactExtractedItem(BaseModel):
    name: str
    website: str
    email: str
    address: str

class ContactExtractRequest(BaseModel):
    location: str
    items: List[ContactExtractItem]

class ContactExtractResult(BaseModel):
    location: str
    items: List[ContactExtractedItem]
    logs: List[str]
    timestamp: str


@app.post("/extract-contacts", response_model=ContactExtractResult)
async def extract_contacts(req: ContactExtractRequest):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç email –∏ –ø–æ—á—Ç–æ–≤—ã–π –∞–¥—Ä–µ—Å —Å —Å–∞–π—Ç–æ–≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π.

    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É, –≤—ã–ø–æ–ª–Ω—è—è –∫—Ä–∞—É–ª–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ,
    —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop. –†–∞–∑—Ä–µ—à–µ–Ω—ã –ø–æ–¥–¥–æ–º–µ–Ω—ã –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ
    –∫–æ—Ä–Ω–µ–≤–æ–≥–æ –¥–æ–º–µ–Ω–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç '–∫–∞–∫ –Ω–∞ —Å–∞–π—Ç–µ'.
    """
    logger.info("‚û°Ô∏è POST /extract-contacts from 127.0.0.1")
    logger.info(f"üßæ Request body: {req.model_dump()}")
    try:
        logs: List[str] = []
        out_items: List[ContactExtractedItem] = []
        items = req.items or []
        for it in items:
            name = (it.name or '').strip()
            website = (it.website or '').strip()
            if website and not website.startswith(('http://', 'https://')):
                website = 'http://' + website
            if not website:
                out_items.append(ContactExtractedItem(name=name, website='', email='', address=''))
                continue
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫—Ä–∞—É–ª–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            logs.append(f"üîç ProxyClient –¥–ª—è {website}: {type(proxy_client) if proxy_client else 'None'}")
            res, crawl_logs = await asyncio.to_thread(
                contacts_crawler.extract_from_site,
                website,
                req.location,
                allow_subdomains=True,
                max_pages=12,
                max_depth=2,
                proxy_client=proxy_client,
            )
            try:
                logs.extend(crawl_logs or [])
            except Exception:
                pass
            email = (res or {}).get('email') or ''
            address = (res or {}).get('address') or ''
            out_items.append(ContactExtractedItem(name=name, website=website, email=email, address=address))

        result = ContactExtractResult(
            location=req.location,
            items=out_items,
            logs=logs,
            timestamp=datetime.now().isoformat()
        )
        logger.info(f"‚¨ÖÔ∏è 200 POST /extract-contacts body: {result.model_dump()}")
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {e}")

@app.get("/", response_class=HTMLResponse)
async def read_root():
    with open("frontend/index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

def _names_file_path(location: str) -> Path:
    key = hashlib.sha1(location.strip().lower().encode('utf-8')).hexdigest()[:16]
    safe_loc = ''.join(ch for ch in location if ch.isalnum() or ch in (' ', '-', '_')).strip().replace(' ', '_')
    fname = f"names_{safe_loc}_{key}.json"
    return NAMES_STORE_DIR / fname

def _write_names_file(location: str, names: List[str]) -> Path:
    path = _names_file_path(location)
    data = {
        'location': location,
        'names': names,
        'timestamp': datetime.now().isoformat()
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

def _read_names_file(location: str) -> Optional[List[str]]:
    path = _names_file_path(location)
    if not path.exists():
        return None
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, dict) and isinstance(data.get('names'), list):
            return data['names']
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è {location}: {e}")
    return None

def _cleanup_temp_names_files(location: Optional[str] = None) -> int:
    """–£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã temp_–Ω–∞–∑–≤–∞–Ω–∏—è_*.xlsx. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω location ‚Äî —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–≥–æ."""
    try:
        patterns: List[str] = []
        if location and location.strip():
            patterns.append(f"temp_–Ω–∞–∑–≤–∞–Ω–∏—è_{location.strip()}_*.xlsx")
        # –í—Å–µ–≥–¥–∞ –ø–æ–¥—á–∏—â–∞–µ–º –æ–±—â–∏–π –º—É—Å–æ—Ä –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
        patterns.append("temp_–Ω–∞–∑–≤–∞–Ω–∏—è_*.xlsx")

        checked_dirs = {PROJECT_ROOT, BACKEND_DIR, Path.cwd()}
        removed = 0
        for d in checked_dirs:
            for pat in patterns:
                for p in d.glob(pat):
                    try:
                        p.unlink()
                        removed += 1
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {p}: {e}")
        if removed:
            logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞–∑–≤–∞–Ω–∏–π: {removed}")
        return removed
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞–∑–≤–∞–Ω–∏–π: {e}")
        return 0

def _normalize(s: str) -> str:
    return ''.join(ch.lower() for ch in s if ch.isalnum() or ch.isspace()).strip()

def _looks_like_same_place(query: str, candidate_name: str) -> bool:
    q = _normalize(query)
    c = _normalize(candidate_name)
    if not q or not c:
        return False
    # —Ç–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
    if q == c:
        return True
    if q in c:
        return True
    # –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º (–≤—Å–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞)
    q_words = [w for w in q.split() if w]
    return all(w in c for w in q_words)

def _validate_location_exists(location: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –Ω–∞—Å–µ–ª—ë–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —á–µ—Ä–µ–∑ Nominatim.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True —Ç–æ–ª—å–∫–æ –¥–ª—è place/boundary —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–∏–ø–æ–≤ –∏ –ø—Ä–∏ —Ä–∞–∑—É–º–Ω–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è.
    """
    try:
        url = "https://nominatim.openstreetmap.org/search"
        params = {"q": location, "format": "jsonv2", "limit": 1, "addressdetails": 1}
        headers = {
            "Accept": "application/json",
            "User-Agent": "ResortContactsCollector/1.0 (contact@example.com)"
        }
        resp = requests.get(url, params=params, headers=headers, timeout=15)
        if resp.status_code != 200:
            raise Exception(f"HTTP {resp.status_code}")
        data = resp.json()
        if not isinstance(data, list) or not data:
            # –§–æ–ª–±—ç–∫ —á–µ—Ä–µ–∑ –Ω–∞—à name_finder (—Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º UA)
            center = None
            try:
                center = name_finder._geocode_city_center(location)  # noqa: SLF001
            except Exception:
                center = None
            return bool(center)
        item = data[0]
        item_class = (item.get("class") or "").lower()
        item_type = (item.get("type") or "").lower()
        allowed_types = {"city", "town", "village", "hamlet", "municipality", "county", "state", "province", "suburb", "locality", "neighbourhood"}
        if item_class not in {"place", "boundary"}:
            return False
        # –¥–ª—è boundary –ø—Ä–∏–Ω–∏–º–∞–µ–º administrative
        if item_class == "boundary" and item_type not in {"administrative"}:
            return False
        if item_class == "place" and item_type and item_type not in allowed_types:
            # –Ω–µ –æ—Ç–≤–µ—Ä–≥–∞–µ–º —Å—Ä–∞–∑—É: —Ñ–æ–ª–±—ç–∫ —á–µ—Ä–µ–∑ –≥–µ–æ—Ü–µ–Ω—Ç—Ä
            center = None
            try:
                center = name_finder._geocode_city_center(location)  # noqa: SLF001
            except Exception:
                center = None
            return bool(center)
        disp = item.get("display_name") or item.get("name") or ""
        if _looks_like_same_place(location, disp):
            return True
        # –§–æ–ª–±—ç–∫: –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ—à–ª–æ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º —Ü–µ–Ω—Ç—Ä
        center = None
        try:
            center = name_finder._geocode_city_center(location)  # noqa: SLF001
        except Exception:
            center = None
        return bool(center)
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞—Å–µ–ª—ë–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞: {e}")
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–æ–ª–±—ç–∫
        try:
            center = name_finder._geocode_city_center(location)  # noqa: SLF001
            return bool(center)
        except Exception:
            return False

@app.post("/list-names", response_model=NameListResult)
async def list_names(request: LocationRequest):
    try:
        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã
        _cleanup_temp_names_files(request.location)
        # 1) –ü—ã—Ç–∞–µ–º—Å—è –±—ã—Å—Ç—Ä–æ –æ—Ç–¥–∞—Ç—å –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –∏ –Ω–µ –ø—É—Å—Ç–æ–π
        cached = _read_names_file(request.location)
        if cached:
            logger.info(f"–û—Ç–¥–∞—é –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞ –¥–ª—è {request.location}: {len(cached)}")
            return NameListResult(location=request.location, names=cached, timestamp=datetime.now().isoformat())

        # 2) –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        start_ts = datetime.now()
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è {request.location}")
        try:
            names = await asyncio.wait_for(asyncio.to_thread(name_finder.find_accommodation_names, request.location), timeout=45)
        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø–æ–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è {request.location}")
            names = []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è {request.location}: {e}")
            names = []
        finally:
            took = (datetime.now() - start_ts).total_seconds()
            logger.info(f"–ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è {request.location} –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {took:.1f}—Å. –ù–∞–π–¥–µ–Ω–æ: {len(names)}")

        # 3) –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–∞—à–ª–∏
        if names:
            _write_names_file(request.location, names)
        return NameListResult(location=request.location, names=names or [], timestamp=datetime.now().isoformat())
    except HTTPException as e:
        # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç—É—Å-–∫–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 404)
        raise e
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π: {str(e)}")

@app.get("/export-names-excel/{location}")
async def export_names_excel_get(location: str):
    """–≠–∫—Å–ø–æ—Ä—Ç —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π (GET) —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —á—Ç–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞."""
    try:
        names = _read_names_file(location) or []
        if not names:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç ‚Äî –Ω–∞—Ö–æ–¥–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º, —á—Ç–æ–±—ã –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –±—ã–ª–æ –±—ã—Å—Ç—Ä–æ
            names = name_finder.find_accommodation_names(location)
            _write_names_file(location, names)

        df = pd.DataFrame({'name': names})
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'–Ω–∞–∑–≤–∞–Ω–∏—è_{location}_{timestamp}.xlsx'
        temp_path = f"temp_{filename}"

        try:
            with pd.ExcelWriter(temp_path, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='–ù–∞–∑–≤–∞–Ω–∏—è')
                ws = writer.sheets['–ù–∞–∑–≤–∞–Ω–∏—è']
                ws.insert_rows(1)
                ws['A1'] = f'–ù–∞–∑–≤–∞–Ω–∏—è –¥–ª—è: {location}'
                ws.insert_rows(2)
                ws['A2'] = f'–î–∞—Ç–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {datetime.now().strftime("%d.%m.%Y %H:%M")}'
                ws.insert_rows(3)
                ws['A3'] = f'–ù–∞–π–¥–µ–Ω–æ: {len(names)}'

            return FileResponse(
                temp_path,
                media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                filename=filename
            )
        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –Ω–∞–∑–≤–∞–Ω–∏–π: {str(e)}")

@app.post("/export-names-excel")
async def export_names_excel_post(req: WebsiteExportRequest):
    """–≠–∫—Å–ø–æ—Ä—Ç —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π (–∏ —Å–∞–π—Ç–æ–≤, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã) —á–µ—Ä–µ–∑ POST."""
    try:
        items = req.items or []
        # –í—Å–µ–≥–¥–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫–∏: name, website, email, address
        rows: List[Dict[str, str]] = []
        for it in items:
            rows.append({
                'name': (it.name or ''),
                'website': ((it.website or '').strip() or '—Å–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω'),
                'email': ((getattr(it, 'email', '') or '').strip() or '–Ω–µ –Ω–∞–π–¥–µ–Ω'),
                'address': ((getattr(it, 'address', '') or '').strip() or '–Ω–µ –Ω–∞–π–¥–µ–Ω'),
            })
        if not rows:
            df = pd.DataFrame(columns=['name', 'website', 'email', 'address'])
        else:
            df = pd.DataFrame(rows, columns=['name', 'website', 'email', 'address'])

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'–Ω–∞–∑–≤–∞–Ω–∏—è_{req.location}_{timestamp}.xlsx'
        temp_path = f"temp_{filename}"
        try:
            with pd.ExcelWriter(temp_path, engine='openpyxl') as writer:
                sheet = '–ù–∞–∑–≤–∞–Ω–∏—è'
                df.to_excel(writer, index=False, sheet_name=sheet)
                ws = writer.sheets[sheet]
                ws.insert_rows(1)
                ws['A1'] = f'–ù–∞–∑–≤–∞–Ω–∏—è –¥–ª—è: {req.location}'
                ws.insert_rows(2)
                ws['A2'] = f'–î–∞—Ç–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {datetime.now().strftime("%d.%m.%Y %H:%M")}'
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞ —á–µ—Ç—ã—Ä–µ –∫–æ–ª–æ–Ω–∫–∏
                ws.merge_cells('A1:D1')
                ws.merge_cells('A2:D2')
            return FileResponse(
                temp_path,
                media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                filename=filename
            )
        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –Ω–∞–∑–≤–∞–Ω–∏–π: {str(e)}")

@app.post("/find-websites", response_model=WebsiteFindResult)
async def find_websites(request: WebsiteFindRequest):
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞–±–æ—Ç—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
        agent = WebsiteFinderAgent(proxy_client=proxy_client)
        items, logs = await asyncio.to_thread(agent.find_for_names, request.location, request.names)
        return WebsiteFindResult(
            location=request.location,
            items=[WebsiteItem(**it) for it in items],
            logs=logs,
            timestamp=datetime.now().isoformat(),
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–∞–π—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–∞–π—Ç–æ–≤: {e}")

# –ù–æ–≤—ã–µ API endpoints –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫—ç—à–µ–º
@app.get("/cache-status/{location}", response_model=CacheStatusResponse)
async def get_cache_status(location: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∫—ç—à–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞"""
    try:
        location_match, cache_data = cache_manager.check_location_match(location)
        
        if not location_match:
            return CacheStatusResponse(
                location_match=False,
                next_stage="names",
                process_status={"names_found": False, "websites_found": False, "contacts_extracted": False},
                organizations_count=0
            )
        
        next_stage = cache_manager.get_next_stage(cache_data)
        process_status = {
            "names_found": cache_data.process_status.names_found,
            "websites_found": cache_data.process_status.websites_found,
            "contacts_extracted": cache_data.process_status.contacts_extracted,
            "last_completed_stage": cache_data.process_status.last_completed_stage,
            "last_stage_status": cache_data.process_status.last_stage_status
        }
        
        return CacheStatusResponse(
            location_match=True,
            next_stage=next_stage,
            process_status=process_status,
            organizations_count=len(cache_data.organizations),
            cache_data={
                "current_location": cache_data.current_location,
                "last_update": cache_data.last_update,
                "organizations": [
                    {
                        "name": org.name,
                        "website": org.website,
                        "email": org.email,
                        "address": org.address
                    }
                    for org in cache_data.organizations
                ]
            }
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∫—ç—à–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∫—ç—à–∞: {e}")

@app.post("/cache-update")
async def update_cache(request: CacheUpdateRequest):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–∞–ø–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
        location_match, cache_data = cache_manager.check_location_match(request.location)
        
        if not location_match:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫—ç—à –¥–ª—è –Ω–æ–≤–æ–≥–æ –≥–æ—Ä–æ–¥–∞
            cache_manager.archive_current_cache()
            cache_data = cache_manager.create_empty_cache(request.location)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —ç—Ç–∞–ø–∞
        cache_data = cache_manager.update_stage_status(cache_data, request.stage, request.status)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã
        if request.organizations:
            cache_data.organizations = [
                Organization(
                    name=org.get("name", ""),
                    website=org.get("website", ""),
                    email=org.get("email", ""),
                    address=org.get("address", "")
                )
                for org in request.organizations
            ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à
        success = cache_manager.save_cache(cache_data)
        
        if success:
            return {"status": "success", "message": "–ö—ç—à –æ–±–Ω–æ–≤–ª–µ–Ω"}
        else:
            raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫—ç—à–∞: {e}")

@app.post("/cache-clear/{location}")
async def clear_cache(location: str):
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞"""
    try:
        cache_manager.archive_current_cache()
        cache_manager.clear_cache()
        logger.info(f"–ö—ç—à –æ—á–∏—â–µ–Ω –¥–ª—è –≥–æ—Ä–æ–¥–∞: {location}")
        return {"status": "success", "message": "–ö—ç—à –æ—á–∏—â–µ–Ω"}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

@app.post("/collect-contacts", response_model=CollectionResult)
async def collect_contacts(request: LocationRequest, background_tasks: BackgroundTasks):
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{request.location.lower().strip()}"
        if cache_key in results_cache:
            cached_result = results_cache[cache_key]
            if datetime.now() - cached_result['timestamp'] < timedelta(hours=CACHE_EXPIRY_HOURS):
                logger.info(f"–í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {request.location}")
                return cached_result['result']
            else:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à
                del results_cache[cache_key]
        
        logs = []
        contacts = []
        
        # –ê–≥–µ–Ω—Ç–Ω—ã–π —Ä–µ–∂–∏–º
        logs.append(f"ü§ñ –ê–≥–µ–Ω—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–±–æ—Ä –¥–ª—è: {request.location}")
        if not proxy_client:
            raise Exception("ProxyAPI –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        agent = ContactAgent(proxy_client=proxy_client)
        agent_contacts, agent_logs = await agent.run(request.location)
        logs.extend(agent_logs)
        contacts = [ContactInfo(**c) for c in agent_contacts]
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = CollectionResult(
            logs=logs,
            contacts=contacts,
            timestamp=datetime.now().isoformat(),
            location=request.location
        )
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results_cache[cache_key] = {
            'result': result,
            'timestamp': datetime.now()
        }
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞
        background_tasks.add_task(cleanup_old_cache)
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {str(e)}")

@app.get("/export-excel/{location}")
async def export_excel(location: str):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel —Ñ–∞–π–ª"""
    try:
        cache_key = f"{location.lower().strip()}"
        if cache_key not in results_cache:
            raise HTTPException(status_code=404, detail="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –Ω–∞—Å–µ–ª–µ–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        cached_data = results_cache[cache_key]
        contacts = cached_data['result'].contacts
        
        if not contacts:
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            df = pd.DataFrame(columns=['name', 'address', 'coordinates', 'email', 'website'])
            logger.info(f"–°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π Excel —Ñ–∞–π–ª –¥–ª—è {location} (0 –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤)")
        else:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            df = pd.DataFrame([contact.model_dump() for contact in contacts])
            logger.info(f"–°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª –¥–ª—è {location} —Å {len(contacts)} –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'–∫–æ–Ω—Ç–∞–∫—Ç—ã_{location}_{timestamp}.xlsx'
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        temp_path = f"temp_{filename}"
        
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            with pd.ExcelWriter(temp_path, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='–ö–æ–Ω—Ç–∞–∫—Ç—ã')
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–±–æ—á—É—é –∫–Ω–∏–≥—É –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                workbook = writer.book
                worksheet = writer.sheets['–ö–æ–Ω—Ç–∞–∫—Ç—ã']
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                worksheet.insert_rows(1)
                worksheet['A1'] = f'–ö–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è: {location}'
                worksheet['A2'] = f'–î–∞—Ç–∞ —Å–±–æ—Ä–∞: {datetime.now().strftime("%d.%m.%Y %H:%M")}'
                worksheet['A3'] = f'–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {len(contacts)}'
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —è—á–µ–π–∫–∏ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                worksheet.merge_cells('A1:E1')
                worksheet.merge_cells('A2:E2')
                worksheet.merge_cells('A3:E3')
            
            logger.info(f"Excel —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {temp_path}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∞–π–ª
            return FileResponse(
                temp_path,
                media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                filename=filename
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel —Ñ–∞–π–ª–∞: {str(e)}")
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel —Ñ–∞–π–ª–∞: {str(e)}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {str(e)}")

@app.post("/export-websites-excel")
async def export_websites_excel(req: WebsiteExportRequest):
    """–≠–∫—Å–ø–æ—Ä—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –≤ Excel –ø–æ —Ç–µ–∫—É—â–µ–º—É —Å–ø–∏—Å–∫—É –∏–º—ë–Ω."""
    try:
        rows = [{"name": it.name, "website": it.website} for it in (req.items or [])]
        df = pd.DataFrame(rows if rows else [], columns=["name", "website"])
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'—Å–∞–π—Ç—ã_{req.location}_{timestamp}.xlsx'
        temp_path = f"temp_{filename}"
        try:
            with pd.ExcelWriter(temp_path, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='–°–∞–π—Ç—ã')
                ws = writer.sheets['–°–∞–π—Ç—ã']
                ws.insert_rows(1)
                ws['A1'] = f'–°–∞–π—Ç—ã –¥–ª—è: {req.location}'
                ws.insert_rows(2)
                ws['A2'] = f'–î–∞—Ç–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {datetime.now().strftime("%d.%m.%Y %H:%M")}'
                ws.merge_cells('A1:B1')
                ws.merge_cells('A2:B2')
            return FileResponse(
                temp_path,
                media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                filename=filename
            )
        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ —Å–∞–π—Ç–æ–≤: {str(e)}")

@app.get("/cache-status")
async def get_cache_status():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫—ç—à–∞"""
    cache_info = {
        'total_entries': len(results_cache),
        'entries': []
    }
    
    for location, data in results_cache.items():
        age_hours = (datetime.now() - data['timestamp']).total_seconds() / 3600
        cache_info['entries'].append({
            'location': location,
            'age_hours': round(age_hours, 2),
            'contacts_count': len(data['result'].contacts),
            'timestamp': data['timestamp'].isoformat()
        })
    
    return cache_info

async def cleanup_old_cache():
    """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞"""
    current_time = datetime.now()
    expired_keys = []
    
    for key, data in results_cache.items():
        if current_time - data['timestamp'] > timedelta(hours=CACHE_EXPIRY_HOURS):
            expired_keys.append(key)
    
    for key in expired_keys:
        del results_cache[key]
        logger.info(f"–£–¥–∞–ª–µ–Ω–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∞—è –∑–∞–ø–∏—Å—å –∫—ç—à–∞: {key}")

async def execute_collection_step(step: CollectionStep, location: str, previous_results: List[str]) -> str:
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    enhanced_prompt = f"""
    {step.prompt}
    
    –ù–∞—Å–µ–ª–µ–Ω–Ω—ã–π –ø—É–Ω–∫—Ç: {location}
    
    –ü—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
    {json.dumps(previous_results, ensure_ascii=False, indent=2)}
    
    –í—ã–ø–æ–ª–Ω–∏ —ç—Ç–æ—Ç —à–∞–≥ –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ.
    """
    
    try:
        if not proxy_client:
            raise Exception("ProxyAPI –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        logger.info(f"=== –í–´–ü–û–õ–ù–ï–ù–ò–ï –®–ê–ì–ê {step.step} ===")
        logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥: {step.step} - {step.description}")
        logger.info(f"–ü—Ä–æ–º–ø—Ç –¥–ª—è —à–∞–≥–∞: {enhanced_prompt}")
        
        response = await proxy_client.chat_completion(
            model="claude-3-5-sonnet-20240620",
            messages=[{"role": "user", "content": enhanced_prompt}],
            max_tokens=1000
        )
        
        logger.info(f"=== –ü–û–õ–£–ß–ï–ù –û–¢–í–ï–¢ –î–õ–Ø –®–ê–ì–ê {step.step} ===")
        logger.info(f"ProxyAPI –æ—Ç–≤–µ—Ç –¥–ª—è —à–∞–≥–∞ –ø–æ–ª—É—á–µ–Ω: {response}")
        try:
            result = response['choices'][0]['message']['content']
        except Exception:
            logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ —à–∞–≥–∞: {response}")
            raise
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —à–∞–≥–∞ {step.step}: {result[:200]}...")
        logger.info(f"–ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —à–∞–≥–∞ {step.step}: {result}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
        logger.info(f"=== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–ù–¢–ê–ö–¢–û–í –ò–ó –®–ê–ì–ê {step.step} ===")
        logger.info(f"–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —à–∞–≥–∞ {step.step}")
        extracted_contacts = contact_extractor.extract_contacts_from_text(result)
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ —à–∞–≥–∞ {step.step}: {extracted_contacts}")
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–Ω—Ç–∞–∫—Ç—ã, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        if any(extracted_contacts.values()):
            enhanced_result = f"""
{result}

--- –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –ö–û–ù–¢–ê–ö–¢–´ ---
Email –∞–¥—Ä–µ—Å–∞: {', '.join(extracted_contacts.get('emails', []))}
–¢–µ–ª–µ—Ñ–æ–Ω—ã: {', '.join(extracted_contacts.get('phones', []))}
–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã: {', '.join(extracted_contacts.get('coordinates', []))}
–ê–¥—Ä–µ—Å–∞: {', '.join(extracted_contacts.get('addresses', []))}
"""
            logger.info(f"–®–∞–≥ {step.step} –∑–∞–≤–µ—Ä—à–µ–Ω —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏")
            return enhanced_result
        
        logger.info(f"–®–∞–≥ {step.step} –∑–∞–≤–µ—Ä—à–µ–Ω –±–µ–∑ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —à–∞–≥–∞ {step.step}: {str(e)}")
        logger.error(f"–¢–∏–ø –æ—à–∏–±–∫–∏ —à–∞–≥–∞: {type(e).__name__}")
        logger.error(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ —à–∞–≥–∞: {e}")
        return f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —à–∞–≥–∞: {str(e)}"

def create_basic_plan(location: str) -> CollectionPlan:
    """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∞ —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"""
    
    basic_steps = [
        CollectionStep(
            step=1,
            description="–ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Å—Ç–∞—Ö —Ä–∞–∑–º–µ—â–µ–Ω–∏—è",
            prompt=f"–ù–∞–π–¥–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Å—Ç–∞—Ö —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –≤ {location} (—Å–∞–π—Ç—ã —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ä—Ç–∞–ª–æ–≤, —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏, –æ—Ç–∑—ã–≤—ã)"
        ),
        CollectionStep(
            step=2,
            description="–°–±–æ—Ä –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π",
            prompt=f"–ò–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑–≤–ª–µ–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ç–µ–ª–µ–π, –±–∞–∑ –æ—Ç–¥—ã—Ö–∞, –ø–∞–Ω—Å–∏–æ–Ω–∞—Ç–æ–≤ –∏ —Å–∞–Ω–∞—Ç–æ—Ä–∏–µ–≤ –≤ {location}"
        ),
        CollectionStep(
            step=3,
            description="–ü–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–æ–≤ –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç",
            prompt=f"–î–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞–π–¥–∏ –∏—Ö –ø–æ—á—Ç–æ–≤—ã–µ –∞–¥—Ä–µ—Å–∞ –∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã"
        ),
        CollectionStep(
            step=4,
            description="–ü–æ–∏—Å–∫ email –∞–¥—Ä–µ—Å–æ–≤",
            prompt=f"–ù–∞–π–¥–∏ email –∞–¥—Ä–µ—Å–∞ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"
        ),
        CollectionStep(
            step=5,
            description="–ü–æ–∏—Å–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∞–π—Ç–æ–≤",
            prompt=f"–ù–∞–π–¥–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"
        )
    ]
    
    return CollectionPlan(steps=basic_steps)

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "hotel-contacts-collector"}

@app.get("/log-path")
async def get_log_path():
    try:
        exists = os.path.exists(LOG_PATH)
        size = os.path.getsize(LOG_PATH) if exists else 0
        return {"path": LOG_PATH, "exists": exists, "size": size}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø—É—Ç–∏ –ª–æ–≥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/logs")
async def get_logs(tail: int = 200):
    try:
        if not os.path.exists(LOG_PATH):
            return {"lines": [], "message": "–§–∞–π–ª –ª–æ–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω", "path": LOG_PATH}
        with open(LOG_PATH, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
        tail = max(1, min(tail, 5000))
        return {"path": LOG_PATH, "lines": [l.rstrip("\n") for l in lines[-tail:]]}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ª–æ–≥–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
